{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/riyanshibohra/Documents/GitHub/metropolitan-climate-profiling\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/riyanshibohra/Documents/GitHub/metropolitan-climate-profiling')\n",
    "print(os.getcwd())  # Verify the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from scripts.data_loader import load_processed_dataset\n",
    "from scripts.modeling import (\n",
    "    prepare_data, train_model, evaluate_model, save_model\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to processed data\n",
    "\n",
    "data_folder = Path().resolve() / \"data\"\n",
    "dallas_path = data_folder / \"enhanced_dallas_with_uhi.csv\"\n",
    "arlington_path = data_folder / \"enhanced_arlington_with_uhi.csv\"\n",
    "denton_path = data_folder / \"enhanced_denton_with_uhi.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas = load_processed_dataset(dallas_path)\n",
    "arlington = load_processed_dataset(arlington_path)\n",
    "denton = load_processed_dataset(denton_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for unified modeling\n",
    "combined_data = pd.concat([dallas, arlington, denton], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (32248, 11)\n",
      "Features dtypes:\n",
      " HourlyDryBulbTemperature     float64\n",
      "HourlyWetBulbTemperature     float64\n",
      "HourlyDewPointTemperature    float64\n",
      "HourlyRelativeHumidity       float64\n",
      "HourlyPrecipitation          float64\n",
      "HourlySeaLevelPressure       float64\n",
      "HourlyStationPressure        float64\n",
      "HourlyWindSpeed              float64\n",
      "HourlyWindDirection          float64\n",
      "Hour                         float64\n",
      "Month                        float64\n",
      "dtype: object\n",
      "Target unique values: ['Medium' 'Low' 'High']\n",
      "Training Features Shape: (22573, 11)\n",
      "Test Features Shape: (9675, 11)\n",
      "Classes: ['High' 'Low' 'Medium']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test, label_encoder = prepare_data(combined_data, target_column='UHI Intensity')\n",
    "\n",
    "# Print feature and target info\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Test Features Shape:\", X_test.shape)\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.94      0.84      0.89       180\n",
      "         Low       0.97      0.97      0.97      2530\n",
      "      Medium       0.98      0.99      0.99      6965\n",
      "\n",
      "    accuracy                           0.98      9675\n",
      "   macro avg       0.97      0.93      0.95      9675\n",
      "weighted avg       0.98      0.98      0.98      9675\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 151    0   29]\n",
      " [   0 2452   78]\n",
      " [   9   84 6872]]\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.92      0.82      0.87       180\n",
      "         Low       0.95      0.98      0.96      2530\n",
      "      Medium       0.99      0.98      0.98      6965\n",
      "\n",
      "    accuracy                           0.98      9675\n",
      "   macro avg       0.95      0.92      0.94      9675\n",
      "weighted avg       0.98      0.98      0.98      9675\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 147    0   33]\n",
      " [   1 2475   54]\n",
      " [  11  138 6816]]\n",
      "\n",
      "=== XGBoost ===\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.93      0.93      0.93       180\n",
      "         Low       0.98      0.98      0.98      2530\n",
      "      Medium       0.99      0.99      0.99      6965\n",
      "\n",
      "    accuracy                           0.99      9675\n",
      "   macro avg       0.97      0.97      0.97      9675\n",
      "weighted avg       0.99      0.99      0.99      9675\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 167    0   13]\n",
      " [   0 2479   51]\n",
      " [  12   50 6903]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    trained_model = train_model(model, X_train, y_train)\n",
    "    evaluate_model(trained_model, X_test, y_test, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis\n",
    "\n",
    "After evaluating three different models (Random Forest, Gradient Boosting, and XGBoost) on the UHI Intensity classification task, here's a comparative analysis:\n",
    "\n",
    "#### XGBoost (Best Performing Model)\n",
    "- Highest overall accuracy at 99%\n",
    "- Most balanced performance across all classes\n",
    "- Best performance for 'High' UHI intensity (93% recall) - crucial for identifying severe urban heat conditions\n",
    "- Excellent precision and recall for all classes (97% macro average)\n",
    "- Lowest misclassification rate between classes\n",
    "\n",
    "#### Random Forest (Second Best)\n",
    "- Good overall accuracy at 98%\n",
    "- Strong performance on 'Low' and 'Medium' classes\n",
    "- Slightly lower performance on 'High' class (84% recall)\n",
    "- Good balance between precision and recall (95% macro average)\n",
    "\n",
    "#### Gradient Boosting\n",
    "- Similar overall accuracy to Random Forest (98%)\n",
    "- Lowest performance on 'High' class (82% recall)\n",
    "- More misclassifications between classes compared to XGBoost\n",
    "- Slightly lower macro average (94%)\n",
    "\n",
    "#### Conclusion\n",
    "XGBoost is recommended as the best model for this task because:\n",
    "1. It shows the most balanced performance across all classes\n",
    "2. Has the highest accuracy for identifying high UHI intensity areas\n",
    "3. Shows the least confusion between different intensity levels\n",
    "4. Demonstrates the best overall metrics (precision, recall, and F1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Modeling Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
